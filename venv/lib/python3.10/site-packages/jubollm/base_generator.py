import os
import json
import requests
import uuid
import logging
import time
import google.generativeai as genai

from langchain_community.llms import HuggingFaceEndpoint
from langchain_openai import OpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from jubollm.common.utils import load_yaml_with_environment

from linkinpark.lib.common.logger import getLogger
from linkinpark.lib.common.secret_accessor import SecretAccessor
from dotenv import load_dotenv

load_dotenv(dotenv_path='/Users/wilburkwan/Desktop/llm-agent/venv/lib/python3.10/site-packages/jubollm/nCopilot/.env')

INIT_SYSTEM_PROMPT = "你是一個人工智慧助理。請依照使用者提出的需求輸出對應的結果。"
APP_ENV = os.environ.get("APP_ENV", "dev")
SERVICE_LOGGER = getLogger(
    name="ai-llm-service",
    labels={"env": APP_ENV}
)

def load_gemini_config(file_path='/Users/wilburkwan/Desktop/llm-agent/jubollm/nCopilot/generator/config/server_init_gemini.yaml'):
    environment = os.environ.copy()
    config = load_yaml_with_environment(file_path, environment)
    return config

class BaseGenerator:
    def __init__(self, service_name: str, llm_source: str = 'local', **kwargs):
        self.llm_source = llm_source
        self.generation_params = kwargs
        self.service_name = service_name
        self.service_id = uuid.uuid4().hex

        if self.llm_source == 'local':
            self.llm = HuggingFaceEndpoint(
                **self.generation_params
            )
            self.auth_token = self._get_token()
            self.auth_headers = {"Authorization": f"Bearer {self.auth_token}"}
            self.llm.client.headers = self.auth_headers
        elif self.llm_source == 'openai':
            self.llm = OpenAI(
                **self.generation_params
            )
        elif self.llm_source == 'gemini':
            gemini_config = load_gemini_config()
            google_api_key = gemini_config.get("GOOGLE_API_KEY")
            print(f"Google API Key: {google_api_key}")
            model_name = gemini_config.get("model_name")
            print(f"model_name: {model_name}")
            
        self._log_service()

    def _get_token(self):
        url = "https://ai-model-dev.jubo.health/api/v1/get-token"
        username = SecretAccessor().access_secret('local-gpu-username')
        password = SecretAccessor().access_secret('local-gpu-password')
        data = {"username": username, "password": password}
        response = requests.post(url, data=json.dumps(data))

        return response.json()['token']

    def _log_service(self):
        if self.llm_source == 'local':
            llm_info_request = requests.get(
                url=self.generation_params['endpoint_url'] + '/info',
                headers=self.auth_headers
            )
            
            llm_info = llm_info_request.json()
            SERVICE_LOGGER.info({
                "message": f"initialized new local service: {self.service_name}",
                "metrics": {"called": 1},
                "labels": {
                    'service_id': self.service_id,
                    'service_name': self.service_name,
                    'llm_source': self.llm_source, 
                    'llm_info': json.dumps(llm_info),
                    'generation_params': json.dumps(self.generation_params)
                }
            })
            
        elif self.llm_source == 'openai':
            SERVICE_LOGGER.info({
                "message": f"initialized new openai service: {self.service_name}",
                "metrics": {"called": 1},
                "labels": {
                    'service_id': self.service_id,
                    'service_name': self.service_name,
                    'llm_source': self.llm_source, 
                    'generation_params': json.dumps(self.generation_params)
                }
            })
        elif self.llm_source == 'gemini':
            SERVICE_LOGGER.info({
                "message": f"initialized new gemini service: {self.service_name}",
                "metrics": {"called": 1},
                "labels": {
                    'service_id': self.service_id,
                    'service_name': self.service_name,
                    'llm_source': self.llm_source, 
                    'generation_params': json.dumps(self.generation_params)
                }
            })

    def build_prompt(self, **kwargs):
        base_prompt = kwargs.pop('base_prompt')
        template = ChatPromptTemplate.from_template(INIT_SYSTEM_PROMPT + "\n" + base_prompt)
        formatted_prompt = template.format_prompt(**kwargs)
        return formatted_prompt.to_string()

    def generate(self, **kwargs):
        formatted_prompt = self.build_prompt(**kwargs)
        logging.info(f"Formatted prompt: {formatted_prompt}")
        
        if self.llm_source == 'gemini':
            gemini_config = load_gemini_config()
            google_api_key = gemini_config.get("GOOGLE_API_KEY")
            genai.configure(api_key=google_api_key)
            model_name = gemini_config.get("model_name")
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(formatted_prompt)
            print(f"Gemini AI response: {response.text}")
            return response.text
        else:
            chain = self.llm | StrOutputParser()
            json_schema = chain.invoke(formatted_prompt)
            return json_schema

