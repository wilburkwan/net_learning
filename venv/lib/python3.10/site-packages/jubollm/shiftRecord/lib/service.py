import os
import re
import uuid
import datetime
from typing import Optional
from concurrent.futures import ThreadPoolExecutor
from time import perf_counter

from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

from jubollm.base_generator import BaseGenerator
from jubollm.shiftRecord.prompts.summary import *
from linkinpark.lib.common.logger import getLogger


APP_ENV = os.environ.get("APP_ENV", "dev")
GENERATION_LOGGER = getLogger(
    name="ai-llm-generation",
    labels={"env": APP_ENV}
)
REQUEST_LOGGER = getLogger(
    name="ai-llm-request",
    labels={"env": APP_ENV}
)


class ShiftRecordService(BaseGenerator):
    def __init__(self, llm_config):
        super().__init__(**llm_config)

    def build_prompt(self, technique: str = 'zero-shot-naive') -> dict:
        """
        Generate prompt based on different scenario 

        Args:
            technique       technique used for prompt; "zero-shot-naive" or "zero-shot-detailed

        Return:
            dict with following items
                "prompt": str - prompt generated
                "technique": str - technique used, same as input arg
        """
        if technique == 'zero-shot-naive':
            template = ChatPromptTemplate.from_messages([
                ("system", INIT_SYSTEM_PROMPT),
                ("user", NAIVE_PROMPT)
            ])
        elif technique == 'zero-shot-detailed':
            template = ChatPromptTemplate.from_messages([
                ("system", INIT_SYSTEM_PROMPT),
                ("user", DETAILED_PROMPT)
            ])
        elif technique == 'few-shot':
            examples = [
                {"modules": {"Record紀錄內容1": "因住民喝完開水後吐嘔，故作以下整體性評估...",
                             "Record紀錄內容2": "今日進行鼻胃管護理...",
                             },
                 "output": "1.因喝開水嘔吐，有做整體性評估。\n2.本日有做鼻胃管護理。"},

            ]
            example_template = ChatPromptTemplate.from_messages([
                ("user", DETAILED_PROMPT),
                ("ai", "{output}"),
            ])
            few_shot_template = FewShotChatMessagePromptTemplate(
                example_prompt=example_template,
                examples=examples
            )
            template = ChatPromptTemplate.from_messages([
                ("system", INIT_SYSTEM_PROMPT),
                few_shot_template,
                ("user", DETAILED_PROMPT),
            ])

        return {
            "prompt": template,
            "technique": technique
        }

    def generate(self, modules: str, prompt_dict: dict, request_id: Optional[str] = None) -> dict:
        """
        Generate llm output based on input text 

        Args:
            modules         nursingnote or family interaction data  
            prompt_dict     dict - prompt with metadata generated from self.build_prompt
            request_id      request_id if it is processed in a request batch

        Return:
            generated org message summary dict with following items
                "generationID": str - generation uid
                "content": str - generated summary text
        """
        time_start = perf_counter()
        generationID = uuid.uuid4().hex
        request_id = request_id if request_id else uuid.uuid4().hex

        # evoke chain
        input_data = {
            'modules': modules
        }
        chain = prompt_dict['prompt'] | self.llm | StrOutputParser()
        summary = chain.invoke(input_data)
        summary = self.edit_summary(summary)
        summary = self.summary2list(summary)

        output = {
            'generationID': generationID,
            'content': summary
        }
        time_end = perf_counter()

        # logging
        GENERATION_LOGGER.info({
            "message": f"{self.service_name} generation",
            "metrics": {"called": 1, "Generate_latency": time_end - time_start},
            "labels": {
                'service_id': self.service_id,
                'generation_id': generationID,
                'request_id': request_id,
                'generation_prompt': prompt_dict['prompt'].format(**input_data),
                'generation_prompt_technique': prompt_dict['technique'],
                'generation_content': " ".join(summary)
            }
        })
        REQUEST_LOGGER.info({
            "message": f"{self.service_name} request {request_id} log",
            "metrics": {"called": 1},
            "labels": {
                'service_id': self.service_id,
                'request_id': request_id,
                'generation_id': generationID
            }
        })

        return output

    def generate_request(self, patient_input):
        """
        Batch generate based on API request
        """

        time_start = perf_counter()
        requestID = uuid.uuid4().hex
        shiftRecordAiSummaryID = uuid.uuid4().hex
        timestamp = datetime.datetime.now().isoformat()
        patient_record = {
            "patient": patient_input.patient,
            "shiftRecordAiSummaryID": shiftRecordAiSummaryID,
            "timestamp": timestamp,
            "llmOutput": {},
            "status_code": 500
        }
        base_log = {
            'request_id': requestID,
            'shiftRecordAiSummaryID': shiftRecordAiSummaryID,
            'patient': patient_input.patient,
            'organization': patient_input.organization,
            'request_date': str(patient_input.date),
            'timestamp': timestamp
        }

        # need to check data
        # generate prompts for batch processing
        preprocess_batch = []
        if patient_input.data['護理記錄']:
            modules = patient_input.data['護理記錄']
            prompt_obj = self.build_prompt(technique='zero-shot-detailed')
            preprocess_batch.append({
                'category': '護理記錄',
                'prompt_obj': prompt_obj,
                'modules': modules,
            })
        if patient_input.data['家屬互動']:
            modules = patient_input.data['家屬互動']
            prompt_obj = self.build_prompt(technique='zero-shot-naive')
            preprocess_batch.append({
                'category': '家屬互動',
                'prompt_obj': prompt_obj,
                'modules': modules,
            })

        # https://github.com/huggingface/text-generation-inference/issues/1008
        with ThreadPoolExecutor(max_workers=8) as executor:
            generations = executor.map(
                self.generate,
                [t['modules'] for t in preprocess_batch],
                [t['prompt_obj'] for t in preprocess_batch],
                [requestID] * len(preprocess_batch)
            )

        for i, gen in enumerate(generations):
            patient_record["llmOutput"][preprocess_batch[i]['category']] = gen

        patient_record["status_code"] = 200
        time_end = perf_counter()
        REQUEST_LOGGER.info({
            "message": f"{self.service_name} request patient {patient_input.patient}",
            "metrics": {"called": 1, "Request_latency": time_end - time_start},
            "labels": {'service_id': self.service_id, **base_log}
        })

        return patient_record

    def edit_summary(self, summary):
        pattern = r'(?:^|\n)(AI: |護理師：|System: )'
        summary = re.sub(pattern, '', summary)

        return summary

    def summary2list(self, summary):
        """
        Convert summary to list let ASP WEB can quickly use
        """
        summary_list = summary.split('\n')
        summary_list = [s for s in summary_list if s]
        return summary_list
