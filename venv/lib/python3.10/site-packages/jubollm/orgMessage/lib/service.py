import os
import re
import uuid
import datetime
from typing import Optional
from concurrent.futures import ThreadPoolExecutor
from time import perf_counter

from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

from jubollm.base_generator import BaseGenerator
from jubollm.orgMessage.lib.query_mongo import query_data
from jubollm.orgMessage.prompts.summary import *
from linkinpark.lib.common.logger import getLogger


APP_ENV = os.environ.get("APP_ENV", "dev")
GENERATION_LOGGER = getLogger(
    name="ai-llm-generation",
    labels={"env": APP_ENV}
)
REQUEST_LOGGER = getLogger(
    name="ai-llm-request",
    labels={"env": APP_ENV}
)


class OrgMessageService(BaseGenerator):
    def __init__(self, llm_config):
        super().__init__(**llm_config)

    def build_prompt(self, technique: str = 'zero-shot') -> dict:
        """
        Generate prompt based on technique selected 

        Args:
            technique       technique used for prompt; "zero-shot" or "few-shot"

        Return:
            dict with following items
                "prompt": str - prompt generated
                "technique": str - technique used, same as input arg
        """
        if technique == 'zero-shot':
            template = ChatPromptTemplate.from_messages([
                ("system", INIT_SYSTEM_PROMPT),
                ("user", BASIC_PROMPT)
            ])
        elif technique == 'few-shot':
            examples = [
                {"patient_data": {"afternoonMentalStatus": "good", "moringMentalStatus": "good"},
                 "summary": "服務使用者早上跟下午精神都很好。"},
            ]
            example_template = ChatPromptTemplate.from_messages([
                ("user", BASIC_PROMPT),
                ("ai", "{summary}"),
            ])
            few_shot_template = FewShotChatMessagePromptTemplate(
                example_prompt=example_template,
                examples=examples
            )
            template = ChatPromptTemplate.from_messages([
                ("system", INIT_SYSTEM_PROMPT),
                few_shot_template,
                ("user", BASIC_PROMPT),
            ])

        return {
            "prompt": template,
            "technique": technique
        }

    def generate(self, patient_data: dict, prompt_dict: dict, request_id: Optional[str] = None) -> dict:
        """
        Generate llm output based on input text 

        Args:
            patient_data    patient data queried from query_mongo.query_data
            prompt_dict     dict - prompt with metadata generated from self.build_prompt
            request_id      request_id if it is processed in a request batch

        Return:
            generated org message summary dict with following items
                "generationID": str - generation uid
                "content": str - generated summary text
        """
        time_start = perf_counter()
        generationID = uuid.uuid4().hex
        request_id = request_id if request_id else uuid.uuid4().hex

        # evoke chain
        prompt_fillin = {
            'patient_data': patient_data['patient_data']
        }
        chain = prompt_dict['prompt'] | self.llm | StrOutputParser()
        summary = chain.invoke(prompt_fillin)
        summary = self.edit_summary(summary, patient_data['patient_demogr'])

        output = {
            'generationID': generationID,
            'content': summary
        }
        time_end = perf_counter()

        # logging
        GENERATION_LOGGER.info({
            "message": f"{self.service_name} generation",
            "metrics": {"called": 1, "Generate_latency": time_end - time_start},
            "labels": {
                'service_id': self.service_id,
                'generation_id': generationID,
                'request_id': request_id,
                'generation_prompt': prompt_dict['prompt'].format(**prompt_fillin),
                'generation_prompt_technique': prompt_dict['technique'],
                'generation_content': summary
            }
        })
        REQUEST_LOGGER.info({
            "message": f"{self.service_name} request {request_id} log",
            "metrics": {"called": 1},
            "labels": {
                'service_id': self.service_id,
                'request_id': request_id,
                'generation_id': generationID
            }
        })

        return output

    def generate_request(self, patient_input):
        """
        Batch generate based on API request
        """
        time_start = perf_counter()
        requestID = uuid.uuid4().hex
        orgMessageAiSummaryID = uuid.uuid4().hex
        timestamp = datetime.datetime.now().isoformat()
        patient_record = {
            "patient": patient_input.patient,
            "orgMessageAiSummaryID": orgMessageAiSummaryID,
            "timestamp": timestamp,
            "llmOutput": {},
            "status_code": 500
        }
        base_log = {
            'request_id': requestID,
            'orgMessageAiSummaryID': orgMessageAiSummaryID,
            'patient': patient_input.patient,
            'organization': patient_input.organization,
            'request_date': str(patient_input.date),
            'timestamp': timestamp
        }

        # query database
        try:
            query_start = perf_counter()
            patient_query = query_data(
                patient_id=patient_input.patient,
                org_id=patient_input.organization,
                date=patient_input.date
            )
            query_end = perf_counter()
        except Exception as err:
            patient_record["status_code"] = 520  # cannot find data
            REQUEST_LOGGER.error({
                "message": f"{self.service_name} error: could not find patient data",
                "metrics": {"called": 1, "error": 1},
                "labels": {**base_log, **{
                    'error': str(err)
                }}
            })
            return patient_record

        # generate prompts from categories
        preprocess_batch = []
        for cat in patient_input.category:
            patient_record['llmOutput'][cat] = []
            patient_info = patient_query.copy()
            if cat != 'overall':
                patient_info['patient_data'] = patient_query['patient_data'][cat]
            for technique in ['few-shot', 'zero-shot']:
                prompt_obj = self.build_prompt(technique=technique)
                preprocess_batch.append({
                    'category': cat,
                    'prompt_obj': prompt_obj,
                    'patient_data': patient_info
                })

        # https://github.com/huggingface/text-generation-inference/issues/1008
        with ThreadPoolExecutor(max_workers=8) as executor:
            generations = executor.map(
                self.generate,
                [t['patient_data'] for t in preprocess_batch],
                [t['prompt_obj'] for t in preprocess_batch],
                [requestID] * len(preprocess_batch)
            )
        for i, gen in enumerate(generations):
            patient_record['llmOutput'][preprocess_batch[i]
                                        ['category']].append(gen)

        patient_record["status_code"] = 200
        time_end = perf_counter()
        REQUEST_LOGGER.info({
            "message": f"{self.service_name} request patient {patient_input.patient}",
            "metrics": {"called": 1, "Request_latency": time_end - time_start, "Query_latency": query_end - query_start},
            "labels": {'service_id': self.service_id, **base_log}
        })

        return patient_record

    def edit_summary(self, summary, patient_demogr):
        pattern = r'(?:^|\n)(AI: |護理師：|System: )'
        summary = re.sub(pattern, '', summary)
        title = self.determine_title(patient_demogr)
        summary = re.sub(r'個案|服務使用者|長輩', title, summary)

        return summary

    def determine_title(self, patient_demogr):
        sex = patient_demogr.get('sex')
        age = patient_demogr.get('age')

        if sex == 'male':
            if age < 50:
                return patient_demogr.get('firstname')
            elif 50 <= age <= 60:
                return '阿伯'
            elif 60 < age:
                return '爺爺'
        elif sex == 'female':
            if age < 50:
                return patient_demogr.get('firstname')
            elif 50 <= age <= 60:
                return '阿姨'
            elif 60 < age:
                return '奶奶'

        return '個案'
