import os
import pandas as pd
import gradio as gr
import asyncio
from dotenv import load_dotenv
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core import CancellationToken

load_dotenv()

conversation_log = []

# è®€å–ç’°å¢ƒè®Šæ•¸ä¸­çš„ API Key
api_key = os.getenv("GEMINI_API_KEY")
model_client = OpenAIChatCompletionClient(model="gemini-1.5-flash-8b", api_key=api_key)

# CSV æ‘˜è¦å‡½å¼
def summarize_csv_in_chunks(file_path, chunk_size=1000, max_chunks=5):
    summaries = []
    try:
        for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
            if i >= max_chunks:
                break
            summary = f"ã€å€å¡Š {i+1}ã€‘\nå‰3è¡Œè³‡æ–™ï¼š\n{chunk.head(3).to_csv(index=False)}\nå€å¡Šè¡Œæ•¸ï¼š{chunk.shape[0]}"
            summaries.append(summary)
    except Exception as e:
        return f"è®€å–æˆ–æ‘˜è¦æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
    return "\n".join(summaries)

async def process_file(file_obj, chat_history):
    global conversation_log
    conversation_log.clear()

    if hasattr(file_obj, "name"):
        file_path = file_obj.name
    else:
        chat_history.append({"role": "system", "content": "ç„¡æ³•å–å¾—ä¸Šå‚³æª”æ¡ˆçš„è·¯å¾‘"})
        yield chat_history, None
        return

    chat_history.append({"role": "system", "content": "CSV è³‡æ–™å·²è®€å–å®Œç•¢ï¼Œé–‹å§‹é€²è¡Œæ‘˜è¦åŠåˆ†æ..."})
    yield chat_history, None

    # Debug è¨Šæ¯
    print("å·²æˆåŠŸè®€å– CSV æª”æ¡ˆï¼Œé–‹å§‹è™•ç†æ¯å€‹å€å¡Š...")

    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=1000)):
        summary_text = f"å€å¡Š {i+1} æ‘˜è¦:\n{chunk.head(3).to_csv(index=False)}\nå€å¡Šè¡Œæ•¸ï¼š{chunk.shape[0]}"
        
        # ç¢ºä¿ AssistantAgent æœ‰åŸ·è¡Œ
        print(f"é–‹å§‹åˆ†æå€å¡Š {i+1} ...")

        analysis_agent = AssistantAgent(
            name=f"analysis_agent_{i+1}",
            model_client=model_client
        )

        chat_history.append({"role": "system", "content": summary_text})
        yield chat_history, None  # **é¡¯ç¤º CSV æ‘˜è¦ï¼Œç¢ºä¿ UI æœƒå³æ™‚æ›´æ–°**

        # **ç¢ºä¿ run_stream() ç”¢ç”Ÿçš„ AI å›æ‡‰èƒ½å³æ™‚é¡¯ç¤º**
        cancellation_token = CancellationToken()
        async for event in analysis_agent.run_stream(
            task=f"è«‹æ ¹æ“šä»¥ä¸‹æ‘˜è¦é€²è¡Œåˆ†æï¼Œè­˜åˆ¥å¯¶å¯¶æ—¥å¸¸è¡Œç‚ºç‰¹å¾µèˆ‡ç…§è­·éœ€æ±‚ï¼Œæ•´ç†é—œéµæ•¸æ“šï¼š\n{summary_text}",
            cancellation_token=cancellation_token  
        ):
            if isinstance(event, TextMessage):
                ai_response = event.content
                chat_history.append({"role": "assistant", "content": ai_response})
                conversation_log.append({"source": "assistant", "content": ai_response})
                
                print(f"ğŸ’¬ Assistant å›æ‡‰ï¼ˆå€å¡Š {i+1}ï¼‰: {ai_response}")  # Debug
                yield chat_history, None  # **ç¢ºä¿ UI æœƒå³æ™‚æ›´æ–° AI å›æ‡‰**

    # **åˆ†æå®Œæˆï¼Œå„²å­˜å°è©±ç´€éŒ„**
    df_log = pd.DataFrame(conversation_log)
    log_file = "conversation_log.csv"
    df_log.to_csv(log_file, index=False, encoding="utf-8-sig")

    chat_history.append({"role": "system", "content": "ğŸ¯ åˆ†æå®Œæˆï¼"})
    print("ğŸ¯ æ‰€æœ‰å€å¡Šåˆ†æå®Œæˆï¼")
    yield chat_history, log_file

def send_user_msg(msg, chat_history):
    chat_history.append({"role": "user", "content": msg})
    return chat_history, ""

with gr.Blocks() as demo:
    gr.Markdown("### AI å¯¶å¯¶ç…§è­·åˆ†æç³»çµ±")

    file_input = gr.File(label="ä¸Šå‚³ CSV")
    chat_display = gr.Chatbot(label="å°è©±ç´€éŒ„", type="messages")
    download_log = gr.File(label="ä¸‹è¼‰å°è©±è¨˜éŒ„")

    start_btn = gr.Button("é–‹å§‹åˆ†æ")

    start_btn.click(
        fn=process_file,
        inputs=[file_input, chat_display],
        outputs=[chat_display, download_log]
    )
demo.queue().launch()
